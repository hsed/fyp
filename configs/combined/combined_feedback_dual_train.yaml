# Purpose and scopes of model:

# 0) use packed sequence always
# 1) Train on GT Keypoints (or PCA Embeddings of GT Keypoints) and GT Action. do this is a loop fashion using
# packed. check if this is same as baseline.
# 2) Train on GT Keypoints (or PCA Embeddings of GT Keypoints) and GT Action.
# Test on predicted keypoints (or pca embeddings of predicted keypoints) using a mapping from a pre-train HPE
# and then pass these to current trained model to predict action. this is pretrained method should show improvements to (3)
# 3) Train on GT Depthmaps to low-dim embeddings (30) or same size as keypoints (63) and then followed by lstm try both
# embedding cases, no keypoint signal or side-information, this should be worse than 2 on action accuracy
# 3) Try simple extension of 2 which uses pre-trained model followed by end-to-end training of fewer layers no feedback information
# 4) Allows from feedback of depth images.

# To use test on pre-train hpe and har set num epochs to 0 and presave to true...

# forward_type in data loader
# forward_type in model
# Avg3DError model

# HPE
# 0525_0129 no cond 100 ep
# 0528_0756 equiprob cond 100 ep
# 0529_0026 new best action cond 100 ep (linear++ layers)

# HAR
# 0525_1627 -- older best
# 0531_0551 (or 0528_1657) -- new best ~ 0.4234 // 0.7234/0.8608 check again seq 120    better top3 acc -- causes better combined baseline 
# 0531_0558 -- very slightly better @ 0.411 (0.414 for seq 120) // 0.7443/0.8556 seq of 300

name: BaselineCombined # for use with tensorboard
n_gpu: 1
dtype: float
arch:
  type: CombinedModel
  args:
    hpe_checkpoint: 'saved/BaselineHPE/0528_0756/model_best.pth' # 0129 is much better!!! #2046 'saved/BaselineHPE/0509_1106/model_best.pth' #'saved/BaselineHPE/0515_1848/model_best.pth'
    har_checkpoint: 'saved/BaselineLSTM/0531_0551/model_best.pth' #0531_1625 0531_0551 saved/BaselineLSTM/0531_1623/model_best.pth #bad 1618;  #1026: best 'saved/BaselineLSTM/0521_1300/model_best.pth' #1346 # this is the manual loop (unrolled) version; for rolled/compact version use 1346 (padding better) or 1300 (no padding)
    pca_checkpoint:
    hpe_args:
      input_channels: 1
      action_cond_ver: 0 # use 0 or 6 => no action cond, 6 => best action cond using film 
      dynamic_cond: false # true -> turn off after X epochs
      pca_components: 30
      dropout_prob: 0.3
      train_mode: true
      init_w: true
      predict_action: false #true ## new
      res_blocks_per_group: 5 # 5 -- orig
      eval_pca_space: false
      train_pca_space: false
      fixed_pca: true # pca weights are untrainable
    har_args:
      in_frame_dim: 64 #30 # 63 # use 30 if using pca!
      num_hidden_layers: 1
      use_unrolled_lstm: false #false #true # true # true: about 14x slower
    forward_type: 0 #0 #3 # 0: Combined (? -> (KeyPts, Action)); 1: HPE ((Depth,[Action]) -> KeyPts); 2: HAR (Depth -> Action); 3: HAR (KeyPts -> Action)
    combined_version: '8d4' #'4d' # '3d' #'2d' #'2b' #'2e' #'2d' # '2c'  #'2b' #'2a' # used with forward_type == 0
    ensure_batchnorm_fixed_eval: true
    ensure_dropout_fixed_eval: true #false # true is better! see exp 0603_1822
    temporal_smoothing: 0.4
data_loader:
  type: CombinedDataLoader
  args:
    data_dir: datasets/hand_pose_action
    dataset_type: train
    batch_size: 4 #8 #16 #4 #8 #16
    shuffle: true
    validation_split: -0.2 # -1.0 -- use this for full test set training # need to change this to -1.0 to test properly in future!
    num_workers: 8
    pad_sequence: false # set to false to make it easier for loss to be applied element-wise
    max_pad_length: 120 #150 #-1 #100 #-1 #100
    randomise_params: false
    use_pca: false #true # false # false => eval_pca_space & train_pca_space => false , similarly for true
    debug: false
    forward_type: 0 #3 #0 # 3
    use_wrist_com: false # test this and set to true also set crop to ver 4 or 5!!
optimizer:
  type: Adam
  args:
    lr: 0.00003 #3.0e-05 is better! see exp 0603_1822 0.0001 #0.0003 #1 #0.003
    weight_decay: 1.0e-5 #0.005 #0
    amsgrad: true # note true is good for lstm but bad for hpe ; false and true are almost same for the right lr for combined training
loss: #mse_seq_and_nll_loss #nll_loss
    type: CombinedSeqLoss
    args:
      alpha: 0.002 #0.0002 #0.00002 # 0.002 #0.0002
      loss_type: 'mse_and_nll'
metrics:
- top1_acc
- top3_acc
- Avg3DError # ONLY USE THIS IF FORWARD TYPE IS 0 FOR EVERYTHING ELSE REMOVE
- mse_only_seq
- nll_only_seq
- combined_only_seq
lr_scheduler: ### new for v4d cause it stagnates at ep 15
  type: StepLR
  args:
    step_size: 12 #50
    gamma: 0.1
trainer:
  epochs: 30 # 100 -- use this when val split == -1.0
  save_dir: saved/
  save_period: 500
  verbosity: 2
  persistent_storage: false #true #false #true # false # this is very helpful when also loading depth images
  monitor: min val_combined_only_seq #min val_avg_3d_err_mm #max val_top1_acc
  early_stop: 10 # 30 -- use this when val split == -1.0 #
  tensorboardX: true
  only_save: false #true #false # true, only do this for when saving the trivial baseline and quit() must quit cause optimiser is at weird state
  no_train: true #false #false
  log_dir: logs